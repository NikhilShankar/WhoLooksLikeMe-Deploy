{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "class WLLMTrainContinuer:\n",
    "\n",
    "    def __init__(self, model, embedding_model, dataset_dir, output_dir):\n",
    "            \"\"\"\n",
    "            Initializes the model with a dataset directory and sets the number of classes.\n",
    "            \"\"\"\n",
    "            self.dataset_dir = dataset_dir\n",
    "            self.train_dir = os.path.join(dataset_dir, 'train')\n",
    "            self.test_dir = os.path.join(dataset_dir, 'test')\n",
    "            # Create the model for classification\n",
    "            self.model = model\n",
    "\n",
    "            #Creat ethe model for embedding\n",
    "            self.embedding_model = embedding_model\n",
    "            \n",
    "\n",
    "    def train_model(self, output_dir, epochs=10, batch_size=32):\n",
    "            \"\"\"\n",
    "            Train the model using the dataset in the folder structure: 'train' and 'test'.\n",
    "            Also save the best model and class names used for training as a CSV file.\n",
    "            Additionally, calculate performance metrics and plot graphs.\n",
    "            \"\"\"\n",
    "            # ImageDataGenerators for loading images\n",
    "            train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                            rotation_range=30, \n",
    "                                            width_shift_range=0.2,\n",
    "                                            height_shift_range=0.2,\n",
    "                                            shear_range=0.2,\n",
    "                                            zoom_range=0.2,\n",
    "                                            horizontal_flip=True)\n",
    "\n",
    "            test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "            # Load training and validation data\n",
    "            train_generator = train_datagen.flow_from_directory(self.train_dir,\n",
    "                                                                target_size=(299, 299),\n",
    "                                                                batch_size=batch_size,\n",
    "                                                                class_mode='categorical')\n",
    "\n",
    "            validation_generator = test_datagen.flow_from_directory(self.test_dir,\n",
    "                                                                    target_size=(299, 299),\n",
    "                                                                    batch_size=batch_size,\n",
    "                                                                    class_mode='categorical')\n",
    "\n",
    "            # Save class names used for training\n",
    "            class_names = train_generator.class_indices  # This gives a dictionary of class names to indices\n",
    "            class_names_list = list(class_names.keys())\n",
    "            \n",
    "            # Convert class names and indices to a pandas DataFrame\n",
    "            class_names_df = pd.DataFrame(list(class_names.items()), columns=[\"Class Name\", \"Class Index\"])\n",
    "            \n",
    "            model_name = os.path.basename(self.dataset_dir)\n",
    "            # Save class names as a CSV file\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            class_names_filepath = os.path.join(output_dir, f'{model_name}.csv')\n",
    "            class_names_df.to_csv(class_names_filepath, index=False)\n",
    "\n",
    "            # Define the callback to save the best model during training\n",
    "            checkpoint = ModelCheckpoint(\n",
    "                os.path.join(output_dir, f'best_model.keras'),  # Save the best model as .h5 file\n",
    "                monitor='val_loss',  # Monitor validation accuracy\n",
    "                verbose=1,  # Print out information when saving the model\n",
    "                save_best_only=True,  # Only save the model if it's the best\n",
    "                mode='min'  # Maximize validation accuracy\n",
    "            )\n",
    "\n",
    "            # Define EarlyStopping callback\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss',         # Monitor validation loss\n",
    "                patience=15,                 # Stop after 3 epochs without improvement\n",
    "                restore_best_weights=True,  # Restore the best model weights after training\n",
    "                verbose=1                   # Print a message when training stops early\n",
    "            )\n",
    "\n",
    "            train_start_time = datetime.now()\n",
    "            # Train the model\n",
    "            history = self.model.fit(\n",
    "                train_generator, \n",
    "                validation_data=validation_generator, \n",
    "                epochs=epochs, \n",
    "                callbacks=[checkpoint, early_stopping]  # Include the checkpoint callback\n",
    "            )\n",
    "            try:\n",
    "                train_total_time = datetime.now() - train_start_time\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                # Save metrics to a pandas DataFrame\n",
    "                train_time_data = {\"Training Classes\" : len(class_names), \"TrainingTime\":train_total_time}\n",
    "                report_df = pd.DataFrame([train_time_data])\n",
    "                report_df.to_csv(os.path.join(output_dir, f'TrainingTimeResults.csv'))\n",
    "\n",
    "                # Plot training & validation accuracy and loss\n",
    "                self.plot_training_history(history, output_dir=output_dir)\n",
    "\n",
    "\n",
    "                val_pred = self.model.predict(validation_generator)\n",
    "                val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "\n",
    "\n",
    "                try:\n",
    "                    # Calculate classification metrics (accuracy, precision, recall, f1-score)\n",
    "                    #true_labels = validation_generator.classes\n",
    "                    #report = classification_report(true_labels, val_pred_classes, target_names=class_names_list, output_dict=True)\n",
    "                    #print(\"Classification Report:\\n\", report)\n",
    "                    #Calculate classification metrics (accuracy, precision, recall, f1-score)\n",
    "                    true_labels = validation_generator.classes\n",
    "                    print(f\"True Labels : {true_labels}\")\n",
    "                    report = {}    \n",
    "                    report = classification_report(true_labels, val_pred_classes, target_names=class_names_list, output_dict=True)\n",
    "                    # Print classification reports for each head\n",
    "                    print(f\"Classification Report for Output:\\n\", report)\n",
    "\n",
    "                    # Save metrics to a pandas DataFrame for each head\n",
    "                    report_df = pd.DataFrame(report).transpose()\n",
    "                    report_df.to_csv(os.path.join(output_dir, f'classification_report.csv'))\n",
    "\n",
    "\n",
    "                    timestamp = datetime.now().strftime(\"%m-%d-%H-%M\")\n",
    "                    # Save results to CSV\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Save metrics to a pandas DataFrame\n",
    "                    report_df = pd.DataFrame(report).transpose()\n",
    "                    report_df.to_csv(os.path.join(output_dir, f'classification_report-{timestamp}.csv'))\n",
    "                except:\n",
    "                    print(\"ERROR IN CREATING CLASSIFICATION REPORT\")\n",
    "\n",
    "\n",
    "                try:\n",
    "                    # Plot confusion matrix\n",
    "                    self.plot_confusion_matrix(true_labels, val_pred_classes, class_names_list, output_dir=output_dir)\n",
    "                except:\n",
    "                    print(\"error in plotting confusion matrix\")\n",
    "\n",
    "            except:\n",
    "                print(\"Error plotting graphs and saving csv\")\n",
    "            return history\n",
    "        \n",
    "\n",
    "    def plot_training_history(self, history, output_dir):\n",
    "        \"\"\"\n",
    "        Plot the training and validation accuracy and loss.\n",
    "        \"\"\"\n",
    "        # Plot training & validation accuracy\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot training & validation loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_dir}/TrainingAndValidationPlot.png', dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, true_labels, pred_labels, class_names, output_dir):\n",
    "        \"\"\"\n",
    "        Plot the confusion matrix using seaborn.\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "        cm_df.to_csv(f'{output_dir}/ConfusionMatrix.csv', index=True)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.savefig(f'{output_dir}/ConfusionMatrixPlot.png', dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def create_embeddings_for_personality(self, data_dir, save_embeddings_dir, should_augment=False, augmentation_count=5):\n",
    "        \"\"\"\n",
    "        Generate embeddings for all images in the provided dataset directory (train) and save the embeddings for each class (folder).\n",
    "        \n",
    "        Parameters:\n",
    "        - data_dir: The root directory where the 'train' folder is located.\n",
    "        - save_embeddings_dir: Directory where the generated embeddings will be saved.\n",
    "        - embedding_dimension: Dimension of the embedding vector (default 128).\n",
    "        - should_augment: Whether to apply augmentation to the images (default True).\n",
    "        - augmentation_count: Number of augmented images to generate per original image (default 5).\n",
    "        \"\"\"\n",
    "        print(f\"Generating embeddings for dataset in {data_dir}\")\n",
    "\n",
    "        # Initialize the ImageDataGenerator for augmentations\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=30,          # Random rotation between -30 and 30 degrees\n",
    "            width_shift_range=0.2,      # Horizontal shift\n",
    "            height_shift_range=0.2,     # Vertical shift\n",
    "            shear_range=0.2,            # Shear transformation\n",
    "            zoom_range=0.2,             # Random zoom\n",
    "            horizontal_flip=True,       # Flip images randomly horizontally\n",
    "            fill_mode='nearest'         # How to fill missing pixels after transformations\n",
    "        )\n",
    "\n",
    "        # Prepare to save embeddings\n",
    "        if not os.path.exists(save_embeddings_dir):\n",
    "            os.makedirs(save_embeddings_dir)\n",
    "\n",
    "        # Iterate through each class folder (personality)\n",
    "        for folder_name in os.listdir(os.path.join(data_dir, 'train')):\n",
    "            folder_path = os.path.join(data_dir, 'train', folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                folder_embeddings = []\n",
    "\n",
    "                # Process each image in the folder (class)\n",
    "                for image_name in os.listdir(folder_path):\n",
    "                    image_path = os.path.join(folder_path, image_name)\n",
    "                    img = load_img(image_path, target_size=(299, 299))  # InceptionResNetV2 input size\n",
    "                    img = img_to_array(img)  # Convert image to array\n",
    "                    img = img / 255.0  # Normalize image to [0, 1]\n",
    "\n",
    "                    if should_augment:\n",
    "                        # Apply augmentation\n",
    "                        augmented_images = datagen.flow(np.expand_dims(img, axis=0), batch_size=1)\n",
    "\n",
    "                        # Generate augmented images and get their embeddings\n",
    "                        augmented_embeddings = []\n",
    "                        for _ in range(augmentation_count):\n",
    "                            augmented_img = next(augmented_images)[0]\n",
    "                            print(augmented_img.shape)\n",
    "                            embedding = self.embedding_model.predict(augmented_img)  # Get embedding from the second output\n",
    "                            augmented_embeddings.append(embedding)\n",
    "\n",
    "                        # Average embeddings for this image (or any other method you prefer)\n",
    "                        embedding = np.mean(np.array(augmented_embeddings), axis=0)\n",
    "                    else:\n",
    "                        embedding = self.embedding_model.predict(np.expand_dims(img, axis=0))  # Get embedding for original image\n",
    "\n",
    "                    folder_embeddings.append(embedding)\n",
    "\n",
    "                # Save the embeddings for the current folder (class)\n",
    "                folder_embeddings = np.array(folder_embeddings)\n",
    "                print(f\"Folder name : {folder_name} :: Embedding Shape : {embedding.shape}\")\n",
    "                embedding_file = os.path.join(save_embeddings_dir, f\"{folder_name}_embedding.npy\")\n",
    "                np.save(embedding_file, folder_embeddings)\n",
    "\n",
    "        print(f\"Embeddings saved to {save_embeddings_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WLLMModelLoader import WLLMModelLoader\n",
    "\n",
    "model_name = \"WLLM-Model-Nikhil-L2-12-10-01-24\" \n",
    "\n",
    "model_info_root_save_dir = \"../SavedTrainingData/savedmodels\"\n",
    "dataset_dir = f\"../TrainingDataImages/{model_name}\"  # Path to your dataset folder\n",
    "savedmodels_dir = f\"{model_info_root_save_dir}/{model_name}\"\n",
    "embeddings_dir = f\"{model_info_root_save_dir}/{model_name}/embeddings\"\n",
    "\n",
    "modelLoader = WLLMModelLoader(f\"{savedmodels_dir}/best_model.keras\")\n",
    "\n",
    "model_continue = WLLMTrainContinuer(modelLoader.model, modelLoader.embedding_model, dataset_dir=dataset_dir, output_dir=savedmodels_dir)\n",
    "model_continue.train_model(output_dir=savedmodels_dir, epochs=50, batch_size=32)\n",
    "model_continue.create_embeddings_for_personality(data_dir=dataset_dir, save_embeddings_dir=embeddings_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WLLMSimilarityCalculatorAdvancedCorrected import SimilarityCalculatorAdvancedCorrected2\n",
    "from WLLMModelLoader import WLLMModelLoader\n",
    "\n",
    "modelLoader2 = WLLMModelLoader(\"../SavedTrainingData/savedmodels/WLLM-Model-Nikhil-C-12-09-22-11/best_model.keras\")\n",
    "similarity_calculator = SimilarityCalculatorAdvancedCorrected2(embeddings_dir)\n",
    "similarity_calculator.calculate_similarity(test_image_path=\"../../test_image/leal.jpg\", model=modelLoader.embedding_model)\n",
    "\n",
    "similarity_calculator2 = SimilarityCalculatorAdvancedCorrected2(\"../SavedTrainingData/savedmodels/WLLM-Model-Nikhil-C-12-09-22-11/embeddings\")\n",
    "similarity_calculator2.calculate_similarity(test_image_path=\"../../test_image/leal.jpg\", model=modelLoader2.embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_calculator.calculate_similarity(test_image_path=\"../../test_image/lail.jpg\", model=modelLoader.embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_calculator2.calculate_similarity(test_image_path=\"../../test_image/lail.jpg\", model=modelLoader2.embedding_model)\n",
    "similarity_calculator.calculate_similarity(test_image_path=\"../../test_image/lail.jpg\", model=modelLoader.embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_calculator.calculate_similarity(test_image_path=\"../../test_image/lail.jpg\", model=modelLoader.embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_calculator2.calculate_similarity(test_image_path=\"../../test_image/bale.jpg\", model=modelLoader2.embedding_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_facenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
